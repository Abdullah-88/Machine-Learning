# -*- coding: utf-8 -*-
"""lecture6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/185VnhymhQol-rBwLnHFR7nALHFSQjK3M
"""

from sklearn.datasets import load_iris
import pandas as pd


iris = load_iris()

data = iris.data
feature_names = iris.feature_names
y = iris.target

df = pd.DataFrame(data,columns = feature_names)
df["species"] = y

x = data

# PCA
from sklearn.decomposition import PCA
pca = PCA(n_components = 2, whiten= True )  # whitten = normalize
pca.fit(x)

x_pca = pca.transform(x)

print("variance ratio: ", pca.explained_variance_ratio_)

print("sum: ",sum(pca.explained_variance_ratio_))

# 2D

df["p1"] = x_pca[:,0]
df["p2"] = x_pca[:,1]

color = ["red","green","blue"]

import matplotlib.pyplot as plt
for each in range(3):
    plt.scatter(df.p1[df.species == each],df.p2[df.species == each],color = color[each],label = iris.target_names[each])
    
plt.legend()
plt.xlabel("p1")
plt.ylabel("p2")
plt.show()

# image reconstruction 
from sklearn.decomposition import PCA
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

mnist = pd.read_csv('/content/sample_data/mnist_train_small.csv')
mnist

mnist.columns

mnist = mnist.drop('6',axis=1)
mnist.shape

scaler = StandardScaler()

mnist = scaler.fit_transform(mnist)

pca = PCA(.95)

lower_dimensional_data = pca.fit_transform(mnist)
pca.n_components_

approximation = pca.inverse_transform(lower_dimensional_data)

plt.figure(figsize=(8,4));

# Original Image
plt.subplot(1, 2, 1);
plt.imshow(mnist[50].reshape(28,28),cmap = plt.cm.gray,interpolation='nearest')
plt.xlabel('784 components', fontsize = 14)
plt.title('Original Image', fontsize = 20);

# 154 principal components
plt.subplot(1, 2, 2);
plt.imshow(approximation[50].reshape(28, 28),
              cmap = plt.cm.gray, interpolation='nearest');
plt.xlabel('154 components', fontsize = 14)
plt.title('95% of Explained Variance', fontsize = 20);

# Explained Variance and number of principal components
pca = PCA()
pca.fit(mnist)
pca.n_components_

# Summing explained variance
tot = sum(pca.explained_variance_)
tot

var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_, reverse=True)] 
print(var_exp[0:5])

# Cumulative explained variance
cum_var_exp = np.cumsum(var_exp)

plt.figure(figsize=(10, 5))
plt.step(range(1, 785), cum_var_exp, where='mid',label='cumulative explained variance')
plt.title('Cumulative Explained Variance as a Function of the Number of Components')
plt.ylabel('Cumulative Explained variance')
plt.xlabel('Principal components')
plt.axhline(y = 95, color='k', linestyle='--', label = '95% Explained Variance')
plt.axhline(y = 90, color='c', linestyle='--', label = '90% Explained Variance')
plt.axhline(y = 85, color='r', linestyle='--', label = '85% Explained Variance')
plt.legend(loc='best')
plt.show()

# Singular value decomposition
import scipy as sp
import numpy as np
from scipy import linalg as LA
A = np.array([[4.0,3.0], [2.0,2.0], [-1.0,-3.0], [-5.0,-2.0]])
U, s, Vt = LA.svd(A, full_matrices=False)
print(U)
#[[-0.61215255 -0.05228813]
# [-0.34162337 -0.2025832 ]
# [ 0.31300005 0.80704816]
# [ 0.64077586 -0.55217683]]
print(s)
#[ 8.16552039 2.30743942]
print(Vt)
#[[-0.81424526 -0.58052102]
# [ 0.58052102 -0.81424526]]
x = np.array([0.243,0.97])
x = x/LA.norm(x)
xi = Vt.dot(x)
print(xi)
#[-0.7609864 -0.64876784]
print(LA.norm(xi))
#1.0
S = LA.diagsvd(s,2,2)
eta = S.dot(xi)
print(eta)
#[-6.21384993 -1.49699248]
print(LA.norm(eta))
#6.391628704703714
S = LA.diagsvd(s,2,2)
eta = S.dot(xi)
print (eta)
y = U.dot(eta)
print(y)
#[ 3.88209899 2.42606187 -3.1530804 -3.15508046]
print(A.dot(x))
#[ 3.88209899 2.42606187 -3.1530804 -3.15508046]