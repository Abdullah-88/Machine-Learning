# -*- coding: utf-8 -*-
"""lecture5_regularization_parameter_selection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F4yvAkZML1weRRn-e6O8kbW4t5vIBrkJ
"""

import numpy as np 
from numpy import linalg as LA
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn import datasets

# Linear Regression 
X,y = datasets.load_diabetes(return_X_y=True)
lr = LinearRegression()
lr.fit(X,y)
scores = cross_val_score(lr, X, y, cv=5,scoring='neg_mean_squared_error') # cv is number of cross validation folds
print(-scores)
# Get mean score and confidence 
print("MSE: %0.2f (+/- %0.2f)" % (-scores.mean(), scores.std() * 2))

# closed form solution of Linear Regression

X,y = datasets.load_diabetes(return_X_y=True)
m,n = X.shape
b = np.ones((m,1))
X = np.hstack((X,b))
kf = KFold(n_splits=5)
cv_error = []
for train_index, test_index in kf.split(X):
  #print("TRAIN:", train_index, "TEST:", test_index)
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]
  w = LA.inv(X_train.T @ X_train ) @ X_train.T @ y_train.T
  predicted = np.dot(X_test,w) 
  MSE = np.mean((y_test - predicted)**2)
  cv_error.append(MSE)
print(cv_error)  
# Get mean score and confidence 
print("MSE: %0.2f (+/- %0.2f)" % (np.mean(cv_error), np.std(cv_error) * 2))

# Regularized Linear Regression with Tikhonov Regularization "Ridge Regression"
X,y = datasets.load_diabetes(return_X_y=True)
regularization_coeffs = np.linspace(0.00001,0.001,100)
print (regularization_coeffs)
mean_cv_score = []
for c in regularization_coeffs:
  ridge = Ridge(alpha=c)
  ridge.fit(X,y)
  scores = cross_val_score(ridge, X, y, cv=5,scoring='neg_mean_squared_error') # cv is number of cross validation folds
  print(-scores)
  # Get mean score and confidence 
  print("MSE: %0.2f (+/- %0.2f)" % (-scores.mean(), scores.std() * 2))
  mean_cv_score.append(-scores.mean())
print("regularization coefficient with minimum error is : ",regularization_coeffs[mean_cv_score.index(min(mean_cv_score))])  
plt.ylabel("cross validation MSE")
plt.xlabel("regularization coefficinet")
plt.plot(regularization_coeffs,mean_cv_score,color= "green")
plt.legend()
plt.show()

# Closed Form Regularized Linear Regression with Tikhonov Regularization "Ridge Regression"
X,y = datasets.load_diabetes(return_X_y=True)
regularization_coeffs = np.linspace(0.00001,0.001,100)
print (regularization_coeffs)
mean_cv_score = []
for c in regularization_coeffs:
  m,n = X.shape
  b = np.ones((m,1))
  X = np.hstack((X,b))
  kf = KFold(n_splits=5)
  cv_error = []
  for train_index, test_index in kf.split(X):
    #print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    w = LA.inv(X_train.T@ X_train + c*np.identity(n+1)) @ X_train.T @ y_train.T
    predicted = np.dot(X_test,w) 
    MSE = np.mean((y_test - predicted)**2)
    cv_error.append(MSE)
    print(cv_error)  
    # Get mean score and confidence 
    print("MSE: %0.2f (+/- %0.2f)" % (np.mean(cv_error), np.std(cv_error) * 2))
  mean_cv_score.append(np.mean(cv_error))
print("regularization coefficient with minimum error is : ",regularization_coeffs[mean_cv_score.index(min(mean_cv_score))])  
plt.ylabel("cross validation MSE")
plt.xlabel("regularization coefficinet")
plt.plot(regularization_coeffs,mean_cv_score,color= "green")
plt.legend()
plt.show()

# Regularized Linear Regression with Lasso Regularization 
X,y = datasets.load_diabetes(return_X_y=True)
regularization_coeffs = np.linspace(0.0001,0.01,100)
print (regularization_coeffs)
mean_cv_score = []
for c in regularization_coeffs:
  lasso = Lasso(alpha=c)
  lasso.fit(X,y)
  scores = cross_val_score(lasso, X, y, cv=5,scoring='neg_mean_squared_error') # cv is number of cross validation folds
  print(-scores)
  # Get mean score and confidence 
  print("MSE: %0.2f (+/- %0.2f)" % (-scores.mean(), scores.std() * 2))
  mean_cv_score.append(-scores.mean())
print("regularization coefficient with minimum error is : ",regularization_coeffs[mean_cv_score.index(min(mean_cv_score))])  
plt.ylabel("cross validation MSE")
plt.xlabel("regularization coefficinet")
plt.plot(regularization_coeffs,mean_cv_score,color= "green")
plt.legend()
plt.show()