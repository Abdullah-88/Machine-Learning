# -*- coding: utf-8 -*-
"""lecture4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tt8UjhH7e75yLRVVCLJIxdatJAdOUuNN
"""

# importing libraries
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import preprocessing

# reading data
train = pd.read_csv("/content/sample_data/california_housing_train.csv")
train.head()
train.shape

test = pd.read_csv("/content/sample_data/california_housing_test.csv")
test.head()
#X_test.shape

# seperating target label from data
X_train = train.drop("median_house_value",axis=1)
y_train = train["median_house_value"]
print(X_train.shape)
print(y_train.shape)

X_test = test.drop("median_house_value",axis=1)
y_test = test["median_house_value"]
print(X_test.shape)
print(y_test.shape)

# standardize data
def standardize(data):
  return (data-np.mean(data)) / np.std(data)

scaler = preprocessing.StandardScaler()
names = X_train.columns
X_train = scaler.fit_transform(X_train)
X_train = pd.DataFrame(X_train,columns=names)
X_train.head()

scaler = preprocessing.StandardScaler()
names = X_test.columns
X_test = scaler.fit_transform(X_test)
X_test = pd.DataFrame(X_test,columns=names)
X_test.head()

# Build the linear regression model
# y = w*X + b
# mean square error = (1/num_samples)*sum (actual - predicted)^2
# gradient = (1/num_samples)*(transposed X *(predicted-actual))
# weight_new = weight_old - learning_rate * gradient of weight
# bias_new =  bias_old - learning_rate * gradient of bias
# define learning rate and number of iterations

# predict function
def predict(X,w,b):
  return np.dot(X,w) + b

# error measure
def mse(actual,predicted):
  return np.mean((actual-predicted)**2)
def SSE(actual,predicted):
  return np.sum((actual - predicted)**2)

# train the model "fitting"
# initialize weight and bias
# make an error list
# iterate to perform gradient decsent
    # inside the for loop:
    # predict
    # calculate gradients
    # update weights and bias using gradients
    # obtain error measure and return results
def fit(X,y,learning_rate,iterations):
  num_samples,num_features = X.shape
  weights = np.zeros(num_features)
  bias = 0.0
  error_history = []
  for iter in range(iterations):
    print ('iteration: ',iter)
    y_predicted = predict(X,weights,bias)

    grad_weights = (1/num_samples)*np.dot(X.T,(y_predicted-y))
    grad_bias =  (1/num_samples)*np.sum(y_predicted-y)

    weights = weights - learning_rate * grad_weights 
    bias =    bias - learning_rate * grad_bias

    error = mse(y,y_predicted)
    print(error)
    error_history.append(error)
  return weights,bias,error_history

# use the obtained model
learning_rate = 0.01
iterations = 1000
w,b,err =fit(X_train,y_train,learning_rate,iterations)
plt.plot(range(iterations),err,color='blue')
plt.show()

print(w)
print(b)

testing = predict(X_test,w,b)
training = predict(X_train,w,b)

measure = np.sqrt(mse(y_test,testing))
print(measure)

plt.scatter(X_train.iloc[:,1],y_train)
plt.scatter(X_train.iloc[:,1], training,color = "red")
plt.show()

plt.scatter(X_test.iloc[:,1],y_test)
plt.scatter(X_test.iloc[:,1], testing,color = "red")
plt.show()

from numpy import linalg as LA
alpha = LA.inv(X_train.T @ X_train ) @ X_train.T @ y_train.T
alpha = LA.lstsq(X_train,y_train)
print(alpha)