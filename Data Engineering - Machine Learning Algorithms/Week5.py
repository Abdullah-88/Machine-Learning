# -*- coding: utf-8 -*-
"""lecture5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zFOqomKAvEMqr1KWdO3i4nmNw-Tb0sHy
"""

#Polynomial Regressin
import pandas as pd
import matplotlib.pyplot as plt
o
df = pd.read_csv("/content/drive/My Drive/SUREYYA HOCA'S COURSE/Machine Learning/3) Polynomial Linear Regression/polynomial-regression.csv",sep = ";")

y = df.araba_max_hiz.values.reshape(-1,1)
x = df.araba_fiyat.values.reshape(-1,1)

plt.scatter(x,y)
plt.ylabel("car_max_velocity")
plt.xlabel("car_price")
plt.show()

# linear regression =  y = b0 + b1*x
# multiple linear regression   y = b0 + b1*x1 + b2*x2

#  linear regression
from sklearn.linear_model import LinearRegression

lr = LinearRegression()

lr.fit(x,y)

# predict
y_head = lr.predict(x)
plt.scatter(x,y)
plt.ylabel("car_max_velocity")
plt.xlabel("car_price")
plt.plot(x,y_head,color="red",label ="linear")
plt.show()

# polynomial regression =  y = b0 + b1*x +b2*x^2 + b3*x^3 + ... + bn*x^n

from sklearn.preprocessing import PolynomialFeatures
polynomial_regression = PolynomialFeatures(degree = 1)

x_polynomial = polynomial_regression.fit_transform(x)


# fit
linear_regression2 = LinearRegression()
linear_regression2.fit(x_polynomial,y)



y_head2 = linear_regression2.predict(x_polynomial)
plt.scatter(x,y)
plt.ylabel("car_max_velocity")
plt.xlabel("car_price")
plt.plot(x,y_head2,color= "green",label = "poly")
plt.legend()
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# Polynomial Regression and Cross Validation
import numpy as np
from numpy import linalg as LA
import scipy as sp
import matplotlib as mpl
import matplotlib.pyplot as plt
# %matplotlib inline

x=[2, 3,   4, 6,  7,    8,  1, 5,   9   ]
y=[6, 8.2, 9, 11, 11.5, 12, 4, 9.5, 11.2]
xT=[2, 3,   4, 6,  7,    8]
yT=[6, 8.2, 9, 11, 11.5, 12]
xE=[1, 5,   9]
yE=[4, 9.5, 11.2]
plt.scatter(xT,yT, s=80, c="blue")
plt.scatter(xE,yE, s=80, c="green")
plt.show()

# set degree
p=1
#learn coefficients on training data xT,yT
coefs=sp.polyfit(xT,yT,p)
ffit = np.poly1d(coefs)
resid = ffit(xE) - yE
print(resid)

SSE = LA.norm(resid)**2
print(SSE)

plt.axis([0,10,0,15])
s=sp.linspace(0,10,101)
plt.plot(s,ffit(s),'r-',linewidth=2.0)
plt.scatter(xT,yT, s=80, c="blue")
plt.scatter(xE,yE, s=50, c="green")
plt.title('degree %s fit | SSE %0.3f' % (p, SSE))
plt.show()

# Regularized Linear Regression
import numpy as np
from numpy import linalg as LA
X = np.array([[1, 8, -3, 5, 4, -9, 4],[1,-2,4,8,-2,-3,2],[1,9,6,-7,4,-5,-5],[1,6,-14,-5,-3,9,-2],[1,-2,11,-6,3,-5,1]])
a = np.array([0,0,0,5,0,-2,0])
noise = np.random.normal(0,0.5,5)
y = X @ a + noise
s = 0.5 # regularization parameter
k = 3 # number of iterations
print("norm: ", LA.norm(y), y)
r = y
for i in range(k):
  # select column index most aligned with residual
  j = np.argmax(abs(r @ X))
  # find best model parameter at j to fit residual
  ajp = (np.dot(r,X[:,j])+s/2) / (LA.norm(X[:,j])**2)
  ajm = (np.dot(r,X[:,j])-s/2) / (LA.norm(X[:,j])**2)
  if LA.norm(r-X[:,j]*ajp) + s*abs(ajp) < LA.norm(r-X[:,j]*ajm) + s*abs(ajm):
    aj = ajp
  else:
    aj = ajm
  # udpate residual
  r = r - X[:,j]*aj
  print("update: ", j, aj, LA.norm(r))
# Ordinary Least Squares
print("OLS: ", LA.inv(X.T @ X) @ X.T @ y.T)
# Ridge Regression
print("ridge: ", LA.inv(X.T@ X + s*np.identity(7)) @ X.T @ y.T)

# Load packages and data set
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import datasets

X, y = datasets.load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)
print (X_train.shape)

# load a model and fit for data using cross validation
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X,y)
from sklearn.model_selection import cross_val_score
scores = cross_val_score(lr, X, y, cv=5)
print(scores)

# Get mean score and confidence 
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))